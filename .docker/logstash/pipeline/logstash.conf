# logstash/pipeline/logstash.conf
input {
  tcp {
    port => 5000
    codec => json_lines
    tcp_keep_alive => true
    dns_reverse_lookup_enabled => false
  }

  udp {
    port => 5000
    codec => json
    buffer_size => 65536
    workers => 4
  }
}

filter {
  # Minimal filtering for maximum throughput
  if ![timestamp] {
    mutate {
      add_field => { "timestamp" => "%{@timestamp}" }
    }
  }

  # Add metrics for monitoring
  metrics {
    meter => "events"
    add_tag => "metric"
    flush_interval => 10
    clear_interval => 60
  }
}

output {
  if "metric" in [tags] {
    stdout {
      codec => line {
        format => "Rate: %{[events][rate_1m]} events/sec"
      }
    }
  } else {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "logs-%{+YYYY.MM.dd}"

      # Performance optimizations
      flush_size => 5000
      idle_flush_time => 1
      pool_max => 1000
      pool_max_per_route => 100
      resurrect_delay => 5
      validate_after_inactivity => 10000
      http_compression => false

      # Inline index settings for ES 8.x/9.x
      manage_template => true
      template_overwrite => true
      template_name => "logs"
      index_patterns => ["logs-*"]

      # Index settings for high throughput
      template_settings => {
        "index.number_of_shards" => 3
        "index.number_of_replicas" => 0
        "index.refresh_interval" => "30s"
        "index.translog.durability" => "async"
        "index.translog.sync_interval" => "30s"
        "index.translog.flush_threshold_size" => "1gb"
      }
    }
  }

  # Also output to stdout for monitoring (comment out in production)
  stdout {
    codec => dots
  }
}